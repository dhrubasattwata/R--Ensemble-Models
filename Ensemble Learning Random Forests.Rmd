---
title: 'Ensemble Modeling: Random Forests'
author: "Dhrubasattwata Roy Choudhury"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Ensemble Modeling: Random Forests
Random forests are a modification of bagged decision trees that build a large collection of de-correlated trees to further improve predictive performance. They have become a very popular “out-of-the-box” or “off-the-shelf” learning algorithm that enjoys good predictive performance with relatively little hyperparameter tuning. Many modern implementations of random forests exist; however, Leo Breiman’s algorithm (Breiman 2001) has largely become the authoritative procedure.

### Importing the packages

```{r}
library(dplyr)    
library(ggplot2)  
library(ranger)   
library(AmesHousing)
library(rsample) 
```

### Datset and Train-Test Split


```{r}
### Dataset and Train-Test Split
ames <- AmesHousing::make_ames()
# Stratified sampling with the rsample package
set.seed(123)
split <- initial_split(ames, prop = 0.8, strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)

```

### Random Forests: Extending bagging
Random forests are built using the same fundamental principles as decision trees and bagging. Bagging trees introduces a random component into the tree building process by building many trees on bootstrapped copies of the training data. Bagging then aggregates the predictions across all the trees; this aggregation reduces the variance of the overall procedure and results in improved predictive performance. However, simply bagging trees results in tree correlation that limits the effect of variance reduction.

Random forests help to reduce tree correlation by injecting more randomness into the tree-growing process More specifically, while growing a decision tree during the bagging process, random forests perform split-variable randomization where each time a split is to be performed, the search for the split variable is limited to a random subset of the original features. Since the algorithm randomly selects a bootstrap sample to train on and a random sample of features to use at each split, a more diverse set of trees is produced which tends to lessen tree correlation beyond bagged trees and often dramatically increase predictive power. 

The basic algorithm for a regression or classification random forest can be generalized as follows:    
1.  Given a training data set  
2.  Select number of trees to build (n_trees)  
3.  for i = 1 to n_trees do  
4.  |  Generate a bootstrap sample of the original data  
5.  |  Grow a regression/classification tree to the bootstrapped data  
6.  |  for each split do  
7.  |  | Select m_try variables at random from all p variables  
8.  |  | Pick the best variable/split-point among the m_try  
9.  |  | Split the node into two child nodes  
10. |  end  
11. | Use typical tree model stopping criteria to determine when a tree is complete (but do not prune)    
12. end  
13. Output ensemble of trees   

Remember, when split variable is limited to a random subset is equal to the original features Random Forests is equivalent to a Bagging. 

### Out-of-the-box performance

Random forests have become popular because they tend to provide very good out-of-the-box performance. Although they have several hyperparameters that can be tuned, the default values tend to produce good results.

### BUILD up the model

```{r}
# number of features
n_features <- length(setdiff(names(ames_train), "Sale_Price"))

# train a default random forest model
ames_rf1 <- ranger(
  Sale_Price ~ ., 
  data = ames_train,
  mtry = floor(n_features / 3),
  respect.unordered.factors = "order",
  seed = 123
)

ames_rf1
```

### Performance Metric: RMSE

```{r}
# get OOB RMSE
default_rmse <- sqrt(ames_rf1$prediction.error)
default_rmse

```

### Hypertuning Parameters
Although random forests perform well out-of-the-box, there are several tunable hyperparameters that we should consider when training a model. Although we briefly discuss the main hyperparameters, Probst, Wright, and Boulesteix (2019) provide a much more thorough discussion. The main hyperparameters to consider include:

1. The number of trees in the forest.   
2. The number of features to consider at any given split.   
3. The complexity of each tree   
4. The sampling scheme   
5. The splitting rule to use during tree construction  

and (2) typically have the largest impact on predictive accuracy and should always be tuned. (3) and (4) tend to have marginal impact on predictive accuracy but are still worth exploring. They also have the ability to influence computational efficiency. (5) tends to have the smallest impact on predictive accuracy and is used primarily to increase computational efficiency.


### Tuning strategies
As we introduce more complex algorithms with greater number of hyperparameters, we should become more strategic with our tuning strategies. One way to become more strategic is to consider how we proceed through our grid search. Up to this point, all our grid searches have been full Cartesian grid searches where we assess every combination of hyperparameters of interest. We could continue to do the same; for example, the next code block searches across 120 combinations of hyperparameter settings.


```{r}
# create hyperparameter grid
hyper_grid <- expand.grid(
  mtry = floor(n_features * c(.05, .15, .25, .333, .4)), # number of features to consider at any given split. 
  min.node.size = c(1, 3, 5, 10), # complexity of each tree 
  replace = c(TRUE, FALSE),    # sampling scheme                             
  sample.fraction = c(.5, .63, .8),                       
  rmse = NA                                               
)

# execute full cartesian grid search
for(i in seq_len(nrow(hyper_grid))) {
  # fit model for ith hyperparameter combination
  fit <- ranger(
    formula         = Sale_Price ~ ., 
    data            = ames_train, 
    num.trees       = n_features * 10, # number of trees in the forest
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$min.node.size[i],
    replace         = hyper_grid$replace[i],
    sample.fraction = hyper_grid$sample.fraction[i],
    verbose         = FALSE,
    seed            = 123,
    respect.unordered.factors = 'order',
  )
  # export OOB error 
  hyper_grid$rmse[i] <- sqrt(fit$prediction.error)
}
```

# Find out the TOP 10 Models
```{r}
# assess top 10 models
hyper_grid %>%
  arrange(rmse) %>%
  mutate(perc_gain = (default_rmse - rmse) / default_rmse * 100) %>%
  head(10)
```


### Feature interpretation
Computing feature importance and feature effects for random forests follow the same procedure as discussed in Section 10.5. However, in addition to the impurity-based measure of feature importance where we base feature importance on the average total reduction of the loss function for a given feature across all trees, random forests also typically include a permutation-based importance measure. In the permutation-based approach, for each tree, the OOB sample is passed down the tree and the prediction accuracy is recorded. Then the values for each variable (one at a time) are randomly permuted and the accuracy is again computed. The decrease in accuracy as a result of this randomly shuffling of feature values is averaged over all the trees for each predictor. The variables with the largest average decrease in accuracy are considered most important.  


### Setting up the Models again with Impurity based and Permutation based Variable Importance
```{r}
# re-run model with impurity-based variable importance
rf_impurity <- ranger(
  formula = Sale_Price ~ ., 
  data = ames_train, 
  num.trees = 2000,
  mtry = 32,
  min.node.size = 1,
  sample.fraction = .80,
  replace = FALSE,
  importance = "impurity",
  respect.unordered.factors = "order",
  verbose = FALSE,
  seed  = 123
)

# re-run model with permutation-based variable importance
rf_permutation <- ranger(
  formula = Sale_Price ~ ., 
  data = ames_train, 
  num.trees = 2000,
  mtry = 32,
  min.node.size = 1,
  sample.fraction = .80,
  replace = FALSE,
  importance = "permutation",
  respect.unordered.factors = "order",
  verbose = FALSE,
  seed  = 123
)

```

### Visualization: VIP

```{r}
p1 <- suppressWarnings(vip::vip(rf_impurity, num_features = 25, bar = FALSE))
p2 <- suppressWarnings(vip::vip(rf_permutation, num_features = 25, bar = FALSE))

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

We can see that the following variables are important: Overall_Qual, Gr_Liv_Area, Neighborhood

### Conclusion
Final thoughts
Random forests provide a very powerful out-of-the-box algorithm that often has great predictive accuracy. They come with all the benefits of decision trees (with the exception of surrogate splits) and bagging but greatly reduce instability and between-tree correlation. And due to the added split variable selection attribute, random forests are also faster than bagging as they have a smaller feature search space at each tree split. However, random forests will still suffer from slow computational speed as your data sets get larger but, similar to bagging, the algorithm is built upon independent steps, and most modern implementations (e.g., ranger) allow for parallelization to improve training time.


